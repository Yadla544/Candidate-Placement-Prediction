# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MP5vzpoRAh2PLQ8hM7y02HAoVBz1VkwE
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as ptd

data=pd.read_csv('Placement.csv')

data.head(20)

data=data.drop('sl_no',axis=1)
data=data.drop('salary',axis=1)

data.head(1)

data["ssc_b"]=data["ssc_b"].astype('category')
data["hsc_b"]=data["hsc_b"].astype('category')
data["degree_t"]=data["degree_t"].astype('category')
data["workex"]=data["workex"].astype('category')
data["specialisation"]=data["specialisation"].astype('category')
data["status"]=data["status"].astype('category')
data["hsc_s"]=data["hsc_s"].astype('category')
data.dtypes

data["ssc_b"]=data["ssc_b"].cat.codes
data["hsc_b"]=data["hsc_b"].cat.codes
data["degree_t"]=data["degree_t"].cat.codes
data["workex"]=data["workex"].cat.codes
data["specialisation"]=data["specialisation"].cat.codes
data["status"]=data["status"].cat.codes
data["hsc_s"]=data["hsc_s"].cat.codes

data

X=data.iloc[:,:-1].values
Y=data.iloc[:,-1].values
Y
X

Y

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2)

data.head()

from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(
    random_state=0,       # ensures reproducibility (same random splits each run
    solver='lbfgs',
    max_iter=1000
).fit(X_train, Y_train)

clf.score(X_test, Y_test) #

# predicting for random value
clf.predict([[0, 87, 0, 95, 0, 2, 78, 2, 0, 0, 1, 0]])

from sklearn.tree import DecisionTreeClassifier

# create the model
clf = DecisionTreeClassifier(
    criterion="gini",   # or "entropy" for information gain
    random_state=0,     # reproducibility
    max_depth=None      # you can limit tree depth to prevent overfitting
)

# train the model
clf.fit(X_train, Y_train)

# evaluate the model
accuracy = clf.score(X_test, Y_test)
print("Decision Tree Accuracy:", accuracy)

# Step: Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score

# create the model
dt_clf = DecisionTreeClassifier(
    criterion="gini",   # you can also try "entropy"
    random_state=0,
    max_depth=None      # can limit depth to avoid overfitting
)

# train the model
dt_clf.fit(X_train, Y_train)

# predictions
Y_pred_dt = dt_clf.predict(X_test)

# evaluation
print("Decision Tree Confusion Matrix:\n", confusion_matrix(Y_test, Y_pred_dt))
print("Decision Tree Accuracy:", accuracy_score(Y_test, Y_pred_dt))

# Step: Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

# create the model
rf_clf = RandomForestClassifier(
    n_estimators=100,   # number of trees
    criterion="gini",
    random_state=0
)

# train the model
rf_clf.fit(X_train, Y_train)

# predictions
Y_pred_rf = rf_clf.predict(X_test)

# evaluation
print("Random Forest Confusion Matrix:\n", confusion_matrix(Y_test, Y_pred_rf))
print("Random Forest Accuracy:", accuracy_score(Y_test, Y_pred_rf))

from sklearn.svm import SVC

# create the model
svm_clf = SVC(
    kernel="linear",
    random_state=0
)

# train the model
svm_clf.fit(X_train, Y_train)


Y_pred_svm = svm_clf.predict(X_test)


print("SVM Confusion Matrix:\n", confusion_matrix(Y_test, Y_pred_svm))
print("SVM Accuracy:", accuracy_score(Y_test, Y_pred_svm))

from google.colab import drive
drive.mount('/content/drive')

